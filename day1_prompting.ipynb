{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSvUZB6W7K4Dovayk9CYR2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eduardodacostasoares/Data_Science/blob/master/day1_prompting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ouxhbUrcz7_N"
      },
      "outputs": [],
      "source": [
        "%pip install -U -q \"google-generativeai>=0.8.3\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from IPython.display import HTML, Markdown, display"
      ],
      "metadata": {
        "id": "GZPAOLENz86W"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "genai.configure(api_key='AIzaSyAxlN8a3FfnwkkBhofXWG3I0U0KzG3xXtM')\n",
        "flash = genai.GenerativeModel('gemini-1.5-flash')\n",
        "response = flash.generate_content(\"Explain AI to me like I'm a kid.\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "V6xh9n590TWT",
        "outputId": "5616c9e6-f2e8-4009-b64c-e7b1d1545397"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imagine you have a super smart computer that can learn things just like you do! That's kind of what AI, or Artificial Intelligence, is.\n",
            "\n",
            "Think about how you learn to ride a bike. First, you see someone else do it. Then, you try it yourself, and you might fall down a few times. But eventually, you practice and get better at it!\n",
            "\n",
            "AI is like a computer that watches lots of things and learns from them. It can learn to recognize pictures, translate languages, even play games! \n",
            "\n",
            "For example, AI can help your parents find the best route to school on a map app, or it can help doctors find the best way to treat you if you get sick.\n",
            "\n",
            "AI is still learning and growing, but it's getting smarter all the time! Just like you, it's constantly discovering new things and figuring out how to make the world a better place. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output length\n",
        "\n",
        "When generating text with an LLM, the output length affects cost and performance. Generating more tokens increases computation, leading to higher energy consumption, latency, and cost.\n",
        "\n",
        "To stop the model from generating tokens past a limit, you can specify the `max_output_tokens` parameter when using the Gemini API. Specifying this parameter does not influence the generation of the output tokens, so the output will not become more stylistically or textually succinct, but it will stop generating tokens once the specified length is reached. Prompt engineering may be required to generate a more complete output for your given limit."
      ],
      "metadata": {
        "id": "L8sIU5sQ_e1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "short_model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash',\n",
        "    generation_config=genai.GenerationConfig(max_output_tokens=200))\n",
        "\n",
        "response = short_model.generate_content('Write a 1000 word essay on the importance of olives in modern society.')\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "TWe8HgiS_dr3",
        "outputId": "7a50463f-84c7-4c33-b062-bea4557506a2"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "## The Enduring Allure of the Olive: A Timeless Treasure in Modern Society\n",
            "\n",
            "From the ancient groves of Greece and Rome to the bustling markets of modern metropolises, the olive has held a place of unparalleled significance throughout human history. More than just a culinary delight, the olive tree and its fruit represent a confluence of culture, health, and economic value, solidifying their enduring importance in modern society.\n",
            "\n",
            "The olive's cultural significance is deeply interwoven with the fabric of human history. Its hardy nature and resilience against harsh climates made it a symbol of peace, abundance, and longevity in ancient civilizations. The olive branch, famously associated with the dove in the biblical story of Noah's Ark, continues to be a universal symbol of peace and reconciliation. In ancient Greece, olives were a vital component of religious ceremonies and athletic competitions, further cementing their importance in the cultural tapestry of the time.\n",
            "\n",
            "The olive's impact on modern society extends beyond its symbolic value. It plays a crucial role\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = short_model.generate_content('Write a short poem on the importance of olives in modern society.')\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "C782sLmf_q3b",
        "outputId": "d915376c-be76-49cb-d9c9-2f46e63bfea6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From ancient groves to modern shelves,\n",
            "The olive thrives, a gift itself.\n",
            "A briny bite, a savory taste,\n",
            "On tables spread, a welcome haste.\n",
            "\n",
            "From salads green to pizzas hot,\n",
            "Its oil, a gleam, a golden dot.\n",
            "In lotions, soaps, a gentle touch,\n",
            "The olive's grace, we love so much.\n",
            "\n",
            "A symbol strong, of peace and might,\n",
            "A timeless fruit, forever bright.\n",
            "So raise a glass, to olives bold,\n",
            "A story told, a treasure old. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "xl6mtZhX0yY1",
        "outputId": "015ae451-f1e9-4d25-9e4c-a781f7b8acaf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Imagine you have a super smart computer that can learn things just like you do! That's kind of what AI, or Artificial Intelligence, is.\n\nThink about how you learn to ride a bike. First, you see someone else do it. Then, you try it yourself, and you might fall down a few times. But eventually, you practice and get better at it!\n\nAI is like a computer that watches lots of things and learns from them. It can learn to recognize pictures, translate languages, even play games! \n\nFor example, AI can help your parents find the best route to school on a map app, or it can help doctors find the best way to treat you if you get sick.\n\nAI is still learning and growing, but it's getting smarter all the time! Just like you, it's constantly discovering new things and figuring out how to make the world a better place. \n"
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Temperature\n",
        "\n",
        "Temperature controls the degree of randomness in token selection. Higher temperatures result in a higher number of candidate tokens from which the next output token is selected, and can produce more diverse results, while lower temperatures have the opposite effect, such that a temperature of 0 results in greedy decoding, selecting the most probable token at each step.\n",
        "\n",
        "Temperature doesn't provide any guarantees of randomness, but it can be used to \"nudge\" the output somewhat."
      ],
      "metadata": {
        "id": "6njXQSwO_ZB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.api_core import retry\n",
        "\n",
        "high_temp_model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash',\n",
        "    generation_config=genai.GenerationConfig(temperature=2.0))\n",
        "\n",
        "\n",
        "# When running lots of queries, it's a good practice to use a retry policy so your code\n",
        "# automatically retries when hitting Resource Exhausted (quota limit) errors.\n",
        "retry_policy = {\n",
        "    \"retry\": retry.Retry(predicate=retry.if_transient_error, initial=10, multiplier=1.5, timeout=300)\n",
        "}\n",
        "\n",
        "for _ in range(5):\n",
        "  response = high_temp_model.generate_content('Pick a random colour... (respond in a single word)',\n",
        "                                              request_options=retry_policy)\n",
        "  if response.parts:\n",
        "    print(response.text, '-' * 25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "l6TnM0B-1kMg",
        "outputId": "44f16b0a-bd79-4d91-d688-c905a7091fe9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blue. \n",
            " -------------------------\n",
            "Purple. \n",
            " -------------------------\n",
            "Indigo \n",
            " -------------------------\n",
            "Purple \n",
            " -------------------------\n",
            "Purple \n",
            " -------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "low_temp_model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash',\n",
        "    generation_config=genai.GenerationConfig(temperature=0.0))\n",
        "\n",
        "for _ in range(5):\n",
        "  response = low_temp_model.generate_content('Pick a random colour... (respond in a single word)',\n",
        "                                             request_options=retry_policy)\n",
        "  if response.parts:\n",
        "    print(response.text, '-' * 25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "id": "-Z8hGP7C9ZRN",
        "outputId": "2d3c633f-b896-4e1b-af5d-9d09e8e872f6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Purple \n",
            " -------------------------\n",
            "Purple \n",
            " -------------------------\n",
            "Purple \n",
            " -------------------------\n",
            "Purple \n",
            " -------------------------\n",
            "Purple \n",
            " -------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Top-K and top-P** ##\n",
        "Like temperature, top-K and top-P parameters are also used to control the diversity of the model's output.\n",
        "\n",
        "Top-K is a positive integer that defines the number of most probable tokens from which to select the output token. A top-K of 1 selects a single token, performing greedy decoding.\n",
        "\n",
        "Top-P defines the probability threshold that, once cumulatively exceeded, tokens stop being selected as candidates. A top-P of 0 is typically equivalent to greedy decoding, and a top-P of 1 typically selects every token in the model's vocabulary.\n",
        "\n",
        "When both are supplied, the Gemini API will filter top-K tokens first, then top-P and then finally sample from the candidate tokens using the supplied temperature.\n",
        "\n",
        "Run this example a number of times, change the settings and observe the change in output."
      ],
      "metadata": {
        "id": "ZqVosEbf-__z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-001',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        # These are the default values for gemini-1.5-flash-001.\n",
        "        temperature=1.0,\n",
        "        top_k=64,\n",
        "        top_p=0.95,\n",
        "    ))\n",
        "\n",
        "story_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\n",
        "response = model.generate_content(story_prompt, request_options=retry_policy)\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "id": "0sqr4KKk9hee",
        "outputId": "4521e70e-2058-4397-b020-22880de99fae"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Bartholomew, a ginger tabby with eyes like melted gold, wasn't your average house cat. He craved more than tuna snacks and sunbeams. He yearned for adventure, for stories to spin with his whiskers. So, when the old, creaking window in the attic swung open one breezy afternoon, Bartholomew knew his chance had arrived.\n\nHe squeezed through the opening, a mischievous grin splitting his whiskered face. The world outside was a vast, sprawling jungle, full of unknown smells and rustling shadows. A tiny, brown sparrow, flitting through the branches, caught Bartholomew’s eye. He chased it, his sleek body weaving through the sun-dappled garden, his tail twitching with excitement.\n\nHis journey took him past a grumpy, old tomcat guarding a patch of wildflowers, past a chattering squirrel burying nuts in a hollow oak, and past a flock of startled pigeons scattering in a flurry of feathers. The world was full of wonders, each one a new tale to be told.\n\nBartholomew, emboldened by his exploration, ventured further. He followed the winding path through a dense thicket, the air thick with the scent of damp earth and pine needles. He climbed a crumbling stone wall, the world stretching out before him like an unfurled map.\n\nSuddenly, he heard a high-pitched cry. A tiny kitten, barely bigger than a ball of yarn, was stuck in a narrow crevice, its meows filled with fear. Bartholomew, his heart thumping, didn't hesitate. He squeezed into the crevice, his lithe body navigating the narrow space. With a gentle nudge, he freed the kitten, who nestled gratefully against his side.\n\nHe carried the kitten back to the garden, placing it safely beneath a sheltering rose bush. The kitten, no longer afraid, rubbed against Bartholomew’s leg, purring with gratitude. It was then Bartholomew realized, he wasn't just a cat seeking adventure, he was a protector, a friend.\n\nAs the sun dipped below the horizon, casting long shadows across the garden, Bartholomew made his way back to the attic window. The world outside, with its dangers and its beauty, held a special place in his heart now. He slipped through the window, carrying the weight of his adventure in his heart, a tale he would spin with his whiskers for years to come.  He was Bartholomew, the brave adventurer, and he knew, deep within, that his adventures had only just begun. \n"
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-001',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        # These are the default values for gemini-1.5-flash-001.\n",
        "        temperature=1.0,\n",
        "        top_k=10,\n",
        "        top_p=0.95,\n",
        "    ))\n",
        "\n",
        "story_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\n",
        "response = model.generate_content(story_prompt, request_options=retry_policy)\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 569
        },
        "id": "jpBbhohh9y28",
        "outputId": "b6a83041-7c69-4393-fad5-1dadba83ed70"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Barnaby wasn't your average house cat. While his brethren napped in sunbeams and chased dust motes, Barnaby yearned for something more. He dreamt of adventures, of soaring through the night sky, of secrets whispered in the rustling leaves. One crisp autumn afternoon, while Mrs. Higgins, his owner, was out on her weekly grocery run, Barnaby saw his chance.\n\nHe snuck out the cat flap, a thrill shooting down his spine. The world outside shimmered with a new kind of magic. He explored the alleyways, his nose twitching at the scent of burnt coffee and damp earth. He scaled a rusty fire escape, marveling at the rooftops spread before him like a miniature cityscape.\n\nThen, he saw it: a magnificent, shimmering silver cat, perched atop a chimney, its green eyes fixed on the setting sun. Barnaby, his heart pounding with admiration, scrambled towards the rooftop.\n\nThe silver cat, regal and aloof, barely acknowledged him. “Welcome, little one,” it purred, its voice a soft rumble. \"I am Luna. Are you ready for an adventure?\"\n\nBarnaby, his whiskers twitching with excitement, nodded eagerly. Luna led him to a hidden passage behind a crumbling brick wall.  They journeyed through a maze of tunnels, the air filled with the whispers of forgotten stories. They emerged into a secret garden, a symphony of color and fragrance hidden from the bustling city.\n\nHere, Barnaby met a band of adventurous felines: a mischievous tabby, a wise old calico, and a sleek black panther, who were all united by a love for adventure and a shared disdain for the mundane.\n\nLuna, their leader, explained that their mission was to guard the ancient secrets of the city, hidden within this magical garden. They were protectors of the balance, ensuring harmony between the human and feline worlds. \n\nBarnaby, thrilled to be part of this secret society, learned to decipher the whispers of the wind, to communicate with the creatures of the night, and to navigate the hidden paths of the city with the grace of a seasoned adventurer.\n\nAs days turned into weeks, Barnaby's life was a whirlwind of excitement. He helped Luna and her band solve mysteries, recover stolen treasures, and protect the garden from encroaching development. He learned to trust his instincts, his feline senses becoming sharper with each passing day.\n\nOne evening, while watching the sunset from a high rooftop, Luna turned to him. \"You are a brave cat, Barnaby,\" she said, her voice soft. \"But your heart belongs elsewhere.\"\n\nBarnaby was surprised. He had grown accustomed to the thrill of adventure, the camaraderie of the band. But Luna was right. He missed Mrs. Higgins, the warmth of her lap, the comforting routine of his life.\n\nHe returned to the alley, the familiar smells grounding him. As he squeezed through the cat flap, he felt a pang of sadness for the adventures he would miss. But he also felt a sense of fulfillment, knowing he had found a part of himself he never knew existed.\n\nFrom then on, Barnaby lived a double life. He remained a pampered house cat, but every full moon, he would slip out under the cover of darkness to join Luna and the band, a seasoned adventurer returning to his clandestine calling. His adventures had changed him, making him a more confident, curious, and adventurous cat, ready for whatever secrets the world held. \n"
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Zero-shot\n",
        "\n",
        "Zero-shot prompts are prompts that describe the request for the model directly.\n",
        "\n",
        "<table align=left>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1gzKKgDHwkAvexG5Up0LMtl1-6jKMKe4g\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
        "  </td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "YLS3cf5tAI0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-001',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        temperature=0.1,\n",
        "        top_p=1,\n",
        "        max_output_tokens=5,\n",
        "    ))\n",
        "\n",
        "zero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
        "Review: \"Her\" is a disturbing study revealing the direction\n",
        "humanity is headed if AI is allowed to keep evolving,\n",
        "unchecked. I wish there were more movies like this masterpiece.\n",
        "Sentiment: \"\"\"\n",
        "\n",
        "response = model.generate_content(zero_shot_prompt, request_options=retry_policy)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "OJz-HMcZ-zBx",
        "outputId": "4fbc2380-92f4-464b-b294-1182f439301c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: **POSITIVE**\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Enum mode\n",
        "\n",
        "The models are trained to generate text, and can sometimes produce more text than you may wish for. In the preceding example, the model will output the label, sometimes it can include a preceding \"Sentiment\" label, and without an output token limit, it may also add explanatory text afterwards.\n",
        "\n",
        "The Gemini API has an [Enum mode](https://github.com/google-gemini/cookbook/blob/main/quickstarts/Enum.ipynb) feature that allows you to constrain the output to a fixed set of values."
      ],
      "metadata": {
        "id": "ay2iIuxhAndV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import enum\n",
        "\n",
        "class Sentiment(enum.Enum):\n",
        "    POSITIVE = \"positive\"\n",
        "    NEUTRAL = \"neutral\"\n",
        "    NEGATIVE = \"negative\"\n",
        "\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-001',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        response_mime_type=\"text/x.enum\",\n",
        "        response_schema=Sentiment\n",
        "    ))\n",
        "\n",
        "response = model.generate_content(zero_shot_prompt, request_options=retry_policy)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "w-PECP_SAbiW",
        "outputId": "e30194dd-09f8-46b9-dfa0-2dcd2601b36b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One-shot and few-shot\n",
        "\n",
        "Providing an example of the expected response is known as a \"one-shot\" prompt. When you provide multiple examples, it is a \"few-shot\" prompt.\n",
        "\n",
        "<table align=left>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1jjWkjUSoMXmLvMJ7IzADr_GxHPJVV2bg\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
        "  </td>\n",
        "</table>\n"
      ],
      "metadata": {
        "id": "KiF_ghhTBMPr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-latest',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        temperature=0.1,\n",
        "        top_p=1,\n",
        "        max_output_tokens=250,\n",
        "    ))\n",
        "\n",
        "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
        "\n",
        "EXAMPLE:\n",
        "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"small\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"cheese\", \"tomato sauce\", \"peperoni\"]\n",
        "}\n",
        "```\n",
        "\n",
        "EXAMPLE:\n",
        "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
        "JSON Response:\n",
        "```\n",
        "{\n",
        "\"size\": \"large\",\n",
        "\"type\": \"normal\",\n",
        "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
        "}\n",
        "\n",
        "ORDER:\n",
        "\"\"\"\n",
        "\n",
        "customer_order = \"Give me a large with cheese & pineapple\"\n",
        "\n",
        "\n",
        "response = model.generate_content([few_shot_prompt, customer_order], request_options=retry_policy)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "id": "eYK6MGyYA7_c",
        "outputId": "1200ac82-fca4-45ee-e4dc-816e97089dc8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "```json\n",
            "{\n",
            "\"size\": \"large\",\n",
            "\"type\": \"normal\",\n",
            "\"ingredients\": [\"cheese\", \"pineapple\"]\n",
            "}\n",
            "``` \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### JSON mode\n",
        "\n",
        "To provide control over the schema, and to ensure that you only receive JSON (with no other text or markdown), you can use the Gemini API's [JSON mode](https://github.com/google-gemini/cookbook/blob/main/quickstarts/JSON_mode.ipynb). This forces the model to constrain decoding, such that token selection is guided by the supplied schema."
      ],
      "metadata": {
        "id": "t_3Rhk5gBkg4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import typing_extensions as typing\n",
        "\n",
        "class PizzaOrder(typing.TypedDict):\n",
        "    size: str\n",
        "    ingredients: list[str]\n",
        "    type: str\n",
        "\n",
        "\n",
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-latest',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        temperature=0.1,\n",
        "        response_mime_type=\"application/json\",\n",
        "        response_schema=PizzaOrder,\n",
        "    ))\n",
        "\n",
        "response = model.generate_content(\"Can I have a large dessert pizza with apple and chocolate\")\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "ztCkRuyDBNY5",
        "outputId": "21eb77ba-3b17-4641-b2b8-66ebbdfef35d"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"ingredients\": [\"apple\", \"chocolate\"], \"size\": \"large\", \"type\": \"dessert\"}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chain of Thought (CoT)\n",
        "\n",
        "Direct prompting on LLMs can return answers quickly and (in terms of output token usage) efficiently, but they can be prone to hallucination. The answer may \"look\" correct (in terms of language and syntax) but is incorrect in terms of factuality and reasoning.\n",
        "\n",
        "Chain-of-Thought prompting is a technique where you instruct the model to output intermediate reasoning steps, and it typically gets better results, especially when combined with few-shot examples. It is worth noting that this technique doesn't completely eliminate hallucinations, and that it tends to cost more to run, due to the increased token count.\n",
        "\n",
        "As models like the Gemini family are trained to be \"chatty\" and provide reasoning steps, you can ask the model to be more direct in the prompt."
      ],
      "metadata": {
        "id": "ey1hg8NqCFF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
        "am 20 years old. How old is my partner? Return the answer immediately.\"\"\"\n",
        "\n",
        "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "response = model.generate_content(prompt, request_options=retry_policy)\n",
        "\n",
        "print(f\"{response.text}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "EmPE6urdBop-",
        "outputId": "8cda6aa9-23e0-4cc9-ac22-8aafe63cbf2a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Here's how to solve this:\n",
            "\n",
            "* **When you were 4:** Your partner was 3 * 4 = 12 years old.\n",
            "* **Age difference:** Your partner is 12 - 4 = 8 years older than you.\n",
            "* **Current age:** Since you are now 20, your partner is 20 + 8 = **28 years old**. \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ReAct: Reason and act\n",
        "\n",
        "In this example you will run a ReAct prompt directly in the Gemini API and perform the searching steps yourself. As this prompt follows a well-defined structure, there are frameworks available that wrap the prompt into easier-to-use APIs that make tool calls automatically, such as the LangChain example from the chapter.\n",
        "\n",
        "To try this out with the Wikipedia search engine, check out the [Searching Wikipedia with ReAct](https://github.com/google-gemini/cookbook/blob/main/examples/Search_Wikipedia_using_ReAct.ipynb) cookbook example.\n",
        "\n",
        "\n",
        "> Note: The prompt and in-context examples used here are from [https://github.com/ysymyth/ReAct](https://github.com/ysymyth/ReAct) which is published under a [MIT license](https://opensource.org/licenses/MIT), Copyright (c) 2023 Shunyu Yao.\n",
        "\n",
        "<table align=left>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/18oo63Lwosd-bQ6Ay51uGogB3Wk3H8XMO\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
        "  </td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "t9XiMQE6CZ5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_instructions = \"\"\"\n",
        "Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
        "Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
        " (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
        "     will return some similar entities to search and you can try to search the information from those topics.\n",
        " (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
        "     so keep your searches short.\n",
        " (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
        "\"\"\"\n",
        "\n",
        "example1 = \"\"\"Question\n",
        "Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
        "\n",
        "Thought 1\n",
        "The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
        "\n",
        "Action 1\n",
        "<search>Milhouse</search>\n",
        "\n",
        "Observation 1\n",
        "Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
        "\n",
        "Thought 2\n",
        "The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
        "\n",
        "Action 2\n",
        "<lookup>named after</lookup>\n",
        "\n",
        "Observation 2\n",
        "Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
        "\n",
        "Thought 3\n",
        "Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
        "\n",
        "Action 3\n",
        "<finish>Richard Nixon</finish>\n",
        "\"\"\"\n",
        "\n",
        "example2 = \"\"\"Question\n",
        "What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
        "\n",
        "Thought 1\n",
        "I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
        "\n",
        "Action 1\n",
        "<search>Colorado orogeny</search>\n",
        "\n",
        "Observation 1\n",
        "The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
        "\n",
        "Thought 2\n",
        "It does not mention the eastern sector. So I need to look up eastern sector.\n",
        "\n",
        "Action 2\n",
        "<lookup>eastern sector</lookup>\n",
        "\n",
        "Observation 2\n",
        "The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
        "\n",
        "Thought 3\n",
        "The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
        "\n",
        "Action 3\n",
        "<search>High Plains</search>\n",
        "\n",
        "Observation 3\n",
        "High Plains refers to one of two distinct land regions\n",
        "\n",
        "Thought 4\n",
        "I need to instead search High Plains (United States).\n",
        "\n",
        "Action 4\n",
        "<search>High Plains (United States)</search>\n",
        "\n",
        "Observation 4\n",
        "The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n",
        "\n",
        "Thought 5\n",
        "High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
        "\n",
        "Action 5\n",
        "<finish>1,800 to 7,000 ft</finish>\n",
        "\"\"\"\n",
        "\n",
        "# Come up with more examples yourself, or take a look through https://github.com/ysymyth/ReAct/"
      ],
      "metadata": {
        "id": "yhLS7a56CKyJ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To capture a single step at a time, while ignoring any hallucinated Observation steps, you will use `stop_sequences` to end the generation process. The steps are `Thought`, `Action`, `Observation`, in that order."
      ],
      "metadata": {
        "id": "Wp-lS0HIDHQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"\"\"Question\n",
        "Who was the youngest author listed on the transformers NLP paper?\n",
        "\"\"\"\n",
        "\n",
        "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "react_chat = model.start_chat()\n",
        "\n",
        "# You will perform the Action, so generate up to, but not including, the Observation.\n",
        "config = genai.GenerationConfig(stop_sequences=[\"\\nObservation\"])\n",
        "\n",
        "resp = react_chat.send_message(\n",
        "    [model_instructions, example1, example2, question],\n",
        "    generation_config=config,\n",
        "    request_options=retry_policy)\n",
        "print(resp.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "Lhdiv3iACo_o",
        "outputId": "7f186382-e2ea-405e-f87d-87e6ae41e078"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought 1\n",
            "I need to find the transformers NLP paper and check the authors.\n",
            "\n",
            "Action 1\n",
            "<search>transformers NLP paper</search>\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now you can perform this research yourself and supply it back to the model."
      ],
      "metadata": {
        "id": "qDMl6bQxDaqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "observation = \"\"\"Observation 1\n",
        "[1706.03762] Attention Is All You Need\n",
        "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
        "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
        "\"\"\"\n",
        "resp = react_chat.send_message(observation, generation_config=config, request_options=retry_policy)\n",
        "print(resp.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "Q2K7-e34DORX",
        "outputId": "58245808-8ff4-45b9-ec37-fabe36d5d768"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thought 2\n",
            "The first paragraph does not list the authors' ages. I need to look up the authors individually and figure out who's the youngest.\n",
            "\n",
            "Action 2\n",
            "<search>Ashish Vaswani</search> \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This process repeats until the `<finish>` action is reached. You can continue running this yourself if you like, or try the [Wikipedia example](https://github.com/google-gemini/cookbook/blob/main/examples/Search_Wikipedia_using_ReAct.ipynb) to see a fully automated ReAct system at work."
      ],
      "metadata": {
        "id": "szYQ-0fLDoYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating code\n",
        "\n",
        "The Gemini family of models can be used to generate code, configuration and scripts. Generating code can be helpful when learning to code, learning a new language or for rapidly generating a first draft.\n",
        "\n",
        "It's important to be aware that since LLMs can't reason, and can repeat training data, it's essential to read and test your code first, and comply with any relevant licenses.\n",
        "\n",
        "<table align=left>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1YX71JGtzDjXQkgdes8bP6i3oH5lCRKxv\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
        "  </td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "yp5higb7Dx27"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-latest',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        temperature=1,\n",
        "        top_p=1,\n",
        "        max_output_tokens=1024,\n",
        "    ))\n",
        "\n",
        "# Gemini 1.5 models are very chatty, so it helps to specify they stick to the code.\n",
        "code_prompt = \"\"\"\n",
        "Write a Python function to calculate the factorial of a number. No explanation, provide only the code.\n",
        "\"\"\"\n",
        "\n",
        "response = model.generate_content(code_prompt, request_options=retry_policy)\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "_x4jaWY-Ddaz",
        "outputId": "fda25a9e-8215-48ec-c5ea-6678d15010f4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "```python\ndef factorial(n):\n  if n == 0:\n    return 1\n  else:\n    return n * factorial(n-1)\n```"
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code execution\n",
        "\n",
        "The Gemini API can automatically run generated code too, and will return the output.\n",
        "\n",
        "<table align=left>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/11veFr_VYEwBWcLkhNLr-maCG0G8sS_7Z\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
        "  </td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "VflYfhb9E7t_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash-latest',\n",
        "    tools='code_execution',)\n",
        "\n",
        "code_exec_prompt = \"\"\"\n",
        "Calculate the sum of the first 14 prime numbers. Only consider the odd primes, and make sure you count them all.\n",
        "\"\"\"\n",
        "\n",
        "response = model.generate_content(code_exec_prompt, request_options=retry_policy)\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "id": "B6wfLZ9-Dnba",
        "outputId": "1c5762f6-7253-499c-f9a3-050e313ae1da"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I will calculate the sum of the first 14 odd prime numbers.\n\nFirst, I will define a function to check if a number is prime. \n\n\n``` python\ndef is_prime(n):\n  \"\"\"\n  Checks if a number is prime.\n  \"\"\"\n  if n <= 1:\n    return False\n  for i in range(2, int(n**0.5) + 1):\n    if n % i == 0:\n      return False\n  return True\n\n```\n```\n\n```\nNow, I will use the function to find the first 14 odd prime numbers:\n\n\n``` python\nprimes = []\ni = 3\nwhile len(primes) < 14:\n  if is_prime(i):\n    primes.append(i)\n  i += 2\n\nprint(f'The first 14 odd primes are: {primes}')\n\n```\n```\nThe first 14 odd primes are: [3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\n\n```\nFinally, I will calculate the sum of the primes: \n\n\n``` python\nsum_of_primes = sum(primes)\nprint(f'The sum of the first 14 odd primes is: {sum_of_primes}')\n\n```\n```\nThe sum of the first 14 odd primes is: 326\n\n```\nTherefore, the sum of the first 14 odd prime numbers is **326**. \n"
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "While this looks like a single-part response, you can inspect the response to see the each of the steps: initial text, code generation, execution results, and final text summary."
      ],
      "metadata": {
        "id": "ak3pSWzsFO9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for part in response.candidates[0].content.parts:\n",
        "  print(part)\n",
        "  print(\"-----\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LIoCaNrE-fB",
        "outputId": "d8884a7f-668a-4313-c5f3-c0d5af04470e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "text: \"I will calculate the sum of the first 14 odd prime numbers.\\n\\nFirst, I will define a function to check if a number is prime. \\n\\n\"\n",
            "\n",
            "-----\n",
            "executable_code {\n",
            "  language: PYTHON\n",
            "  code: \"\\ndef is_prime(n):\\n  \\\"\\\"\\\"\\n  Checks if a number is prime.\\n  \\\"\\\"\\\"\\n  if n <= 1:\\n    return False\\n  for i in range(2, int(n**0.5) + 1):\\n    if n % i == 0:\\n      return False\\n  return True\\n\"\n",
            "}\n",
            "\n",
            "-----\n",
            "code_execution_result {\n",
            "  outcome: OUTCOME_OK\n",
            "}\n",
            "\n",
            "-----\n",
            "text: \"Now, I will use the function to find the first 14 odd prime numbers:\\n\\n\"\n",
            "\n",
            "-----\n",
            "executable_code {\n",
            "  language: PYTHON\n",
            "  code: \"\\nprimes = []\\ni = 3\\nwhile len(primes) < 14:\\n  if is_prime(i):\\n    primes.append(i)\\n  i += 2\\n\\nprint(f\\'The first 14 odd primes are: {primes}\\')\\n\"\n",
            "}\n",
            "\n",
            "-----\n",
            "code_execution_result {\n",
            "  outcome: OUTCOME_OK\n",
            "  output: \"The first 14 odd primes are: [3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\\n\"\n",
            "}\n",
            "\n",
            "-----\n",
            "text: \"Finally, I will calculate the sum of the primes: \\n\\n\"\n",
            "\n",
            "-----\n",
            "executable_code {\n",
            "  language: PYTHON\n",
            "  code: \"\\nsum_of_primes = sum(primes)\\nprint(f\\'The sum of the first 14 odd primes is: {sum_of_primes}\\')\\n\"\n",
            "}\n",
            "\n",
            "-----\n",
            "code_execution_result {\n",
            "  outcome: OUTCOME_OK\n",
            "  output: \"The sum of the first 14 odd primes is: 326\\n\"\n",
            "}\n",
            "\n",
            "-----\n",
            "text: \"Therefore, the sum of the first 14 odd prime numbers is **326**. \\n\"\n",
            "\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explaining code\n",
        "\n",
        "The Gemini family of models can explain code to you too.\n",
        "\n",
        "<table align=left>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://aistudio.google.com/prompts/1N7LGzWzCYieyOf_7bAG4plrmkpDNmUyb\"><img src=\"https://ai.google.dev/site-assets/images/marketing/home/icon-ais.png\" style=\"height: 24px\" height=24/> Open in AI Studio</a>\n",
        "  </td>\n",
        "</table>"
      ],
      "metadata": {
        "id": "SnfYEQAuFfgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n",
        "\n",
        "explain_prompt = f\"\"\"\n",
        "Please explain what this file does at a very high level. What is it, and why would I use it?\n",
        "\n",
        "```\n",
        "{file_contents}\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
        "\n",
        "response = model.generate_content(explain_prompt, request_options=retry_policy)\n",
        "Markdown(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        },
        "id": "QsQCnvDBFTrF",
        "outputId": "87d6818d-ed1c-453d-83f8-132049d796be"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "This file, a Bash script, provides a framework for enhancing your Bash prompt to display information about your current Git repository.  Here's a high-level breakdown:\n\n**What it is:**\n\n* **Git Prompt:** A collection of functions designed to dynamically modify your Bash prompt to include details about your Git repository, such as:\n    * Branch name\n    * Upstream status (ahead/behind)\n    * Untracked files\n    * Staged changes\n    * And more\n\n* **Customization:** The script offers numerous options for customizing the appearance and information displayed in your prompt.\n\n**Why you would use it:**\n\n* **Git Workflow Awareness:** Provides visual cues about your Git repository's state, making it easier to:\n    *  Track branch changes\n    *  See if you're ahead or behind the upstream branch\n    *  Quickly identify uncommitted changes\n    *  Gain a clearer overview of your Git workflow\n\n* **Efficiency:** By displaying this information directly in your prompt, you can save time and effort by avoiding the need to constantly run `git status` or other Git commands.\n\n**How it works:**\n\n1. **Initialization:** The script defines several functions and variables, including:\n    * **`setGitPrompt()`:** This function is called during your prompt setup to collect Git status information and update your prompt accordingly.\n    * **`git_prompt_config()`:**  Handles loading color settings and themes from configuration files.\n    * **`updatePrompt()`:**  Determines which Git status information to display based on your configuration and current repository state.\n\n2. **Integration:** The script uses the `PROMPT_COMMAND` variable to call `setGitPrompt()` before each prompt, ensuring that your prompt is constantly updated.\n\n3. **Customization:** You can adjust a wide array of settings to personalize the prompt, including:\n    * **Color schemes:**  Use themes or create your own custom themes to set the colors for different prompt elements.\n    * **Displayed information:** Control which Git status details are displayed in your prompt (e.g., upstream status, uncommitted changes, etc.).\n    * **Symbol sets:** Customize the symbols used to represent different Git states (e.g., \"ahead,\" \"behind,\" etc.).\n\n**To use it:**\n\n1. **Save the script:**  Save the provided code as a file (e.g., `git-prompt.sh`).\n2. **Source the script:** Add the following line to your Bash profile (`.bashrc` or `.zshrc`) to make the script's functions available in your shell:\n\n   ```bash\n   source /path/to/git-prompt.sh \n   ```\n\n   Replace `/path/to/git-prompt.sh` with the actual location of the saved file.\n3. **Customize:**  Adjust the script's configuration settings to your preferences (see the comments in the script for more details).\n4. **Restart your shell:**  Log out and back in or run `source ~/.bashrc` (or `source ~/.zshrc`) for the changes to take effect.\n\nNow, your Bash prompt will display dynamic Git status information! \n"
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q8qz7O1sFjWs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}